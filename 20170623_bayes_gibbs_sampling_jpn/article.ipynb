{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ギブスサンプリングでベイズ推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ギブスサンプリングは確率分布からのサンプリング手法の一つ、各変数について条件付き確率さえわかればサンプリングできる。ベイズ推定での事後分布サンプリングに使われることが有る([Just another Gibbs sampler](https://en.wikipedia.org/wiki/Just_another_Gibbs_sampler)というそのものズバリなベイズ推定のオープンソースプロダクトもある)。\n",
    "\n",
    "以下はギブスサンプリングに関する参考文献。2は可視化もあって大変分かりやすい。直感的に理解できるのがギブスサンプリングのいいところだと思う。\n",
    "\n",
    "1. [Gibbs_sampling, wikipedia](https://en.wikipedia.org/wiki/Gibbs_sampling)\n",
    "2. [可視化で理解するマルコフ連鎖モンテカルロ法(MCMC), ほくそ笑む](http://d.hatena.ne.jp/hoxo_m/20140911/p1)\n",
    "\n",
    "ここではギブスサンプリングを実装してベイズ推定を実施する。ギブスサンプリングは簡単なアルゴリズムなので、多分、実装も簡単であろうと思う。\n",
    "\n",
    "実装は以下(Campbell)を参考にした。これを読めば誰でもギブスサンプリングでベイズ推定ができる、とても分かりやすい記事。\n",
    "\n",
    "[Gibbs sampling for Bayesian linear regression in Python, Kieran Campbell](http://kieranrcampbell.github.io/gibbs-sampling-bayesian-linear-regression/)\n",
    "\n",
    "テストデータ、モデルは以下の記事(Memorandum)のものをそのまま利用した。記事中のRstanのコードは推定結果の答え合わせに利用した。このブログは好奇心をそそられる記事に溢れており、眺めるだけで賢くなった気分になれる。\n",
    "\n",
    "[WAICとWBICを事後分布から計算する, StatModeling Memorandum](http://statmodeling.hatenablog.com/entry/calc-waic-wbic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データと統計モデル\n",
    "\n",
    "2成分混合正規分布から100点を生成しテストデータとする。MemorandumがRを使っているので、それにならう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "\n",
    "R = Popen([\"R\", \"--vanilla\", \"--silent\"], stdin=PIPE)\n",
    "\n",
    "R.stdin.write(b'''\n",
    "set.seed(1)\n",
    "N      <- 100\n",
    "a_true <- 0.4\n",
    "mean0  <- 0\n",
    "mean1  <- 3\n",
    "sd0    <- 1\n",
    "sd1    <- 1\n",
    "Y      <- c(rnorm((1-a_true)*N, mean1, sd1), rnorm(a_true*N, mean2, sd2))\n",
    "data   <- list(N=N, Y=Y)\n",
    "\n",
    "write.table(Y, file=\"points.csv\", sep=\",\", row.names=F, col.names=F)\n",
    "''')\n",
    "R.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "統計モデルは2つ。それぞれについてベイズ推定を行う。\n",
    "\n",
    "1. 単一の正規分布モデル\n",
    "2. 2成分混合正規分布モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonセットアップ\n",
    "\n",
    "CampbellにならってPython3を用いる。pandasというのはRのdata.frame的なものらしい。多彩なデータ加工ができるようだが趣味人には使い方が分からない。Anacondaは怖いので使わない。\n",
    "\n",
    "```sh\n",
    "$ python -m venv .venv\n",
    "$ source .venv/bin/activate\n",
    "$ pip install numpy scipy pandas seaborn\n",
    "```\n",
    "\n",
    "以下はpythonの共通設定的なもの。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "import seaborn as sns\n",
    "from seaborn import plt\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可視化のテスト、R経由で作成したテストデータを可視化してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.read_csv(\"points.csv\", header=None)\n",
    "p = sns.distplot(Y, bins=20)\n",
    "plt.savefig(\"points.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "混合分布ってのはこういう形をしているようだ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ギブスサンプリングでベイズ推定のおさらい\n",
    "\n",
    "Campbellの記事でギブスサンプリングを復習。\n",
    "\n",
    "今手元に $N$ 個のモデルパラメタ $ \\theta_j, j=1,\\ldots,n $ とデータ $ y $ があり、ここから事後分布 $ p(\\theta_j \\mid y) $を得たいとする。ギブスサンプリングで事後分布をサンプリングするには、条件付き確率$ p(\\theta_j\\mid\\theta_1, \\ldots, \\theta_{j-1}, \\theta_{j+1}, \\ldots, \\theta_n, y) $ を用いて以下の手順をとる。\n",
    "\n",
    "1. 適当な初期値$ \\theta_j^{(i)}$ を与える.\n",
    "2. すべての$ j$について $ \\theta_j^{(i+1)} \\sim p(\\theta_j\\mid\\theta_1^{(i+1)},\\ldots, \\theta_{j-1}^{(i+1)},\\theta_{j+1}^{(i)}, \\ldots, \\theta_n^{(i)}, y) $ をサンプリング.\n",
    "3. 2.を予め決めたイテレーション数だけ繰り返す.\n",
    "\n",
    "イテレーション数が十分であれば、各パラメタのサンプリング結果の密度分布を、各パラメタの周辺化した事後分布として扱うことができる、らしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確率モデルの実装(モデル1: 単一の正規分布モデル)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの概要\n",
    "\n",
    "ここではデータ $ y $ が従う正規分布の平均 $ \\mu $ と標準偏差 $ \\sigma $ を推定する。\n",
    "\n",
    "$$y_i \\sim \\mathcal{N}(\\mu, \\sigma^2), i=1,\\ldots, N $$\n",
    "\n",
    "データ $ y $ の条件付き確率。\n",
    "\n",
    "$$p(y_1, \\ldots, y_N \\mid \\mu, \\sigma) = \\prod_{i = 1}^{N} \\mathcal{N}(y_i\\mid \\mu, \\sigma^2)$$\n",
    "\n",
    "パラメタには事前分布は、簡単のため無情報事前分布とする。\n",
    "\n",
    "$$\\mu \\sim \\rm{uniform}(-\\infty, \\infty) $$ $$\\sigma \\sim \\rm{uniform}(0, \\infty) $$\n",
    "\n",
    "このときパラメタの確率は一定になる。\n",
    "\n",
    "$$ p(\\mu)=\\rm {constant} $$ $$ p(\\sigma) = \\rm {constant} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\mu $ のサンプリング\n",
    "\n",
    "何はなくとも条件付き確率がわからなければサンプリングできない。 定数項と$ \\mu $ に独立な項を消去する。\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "p(\\mu \\mid \\sigma, y) &=& \\frac {p(\\mu, \\sigma, y)} {p(\\sigma, y)} \\\\\n",
    "&\\propto& p(y \\mid \\mu, \\sigma) p(\\mu) p(\\sigma) \\\\\n",
    "&\\propto& p(y \\mid \\mu, \\sigma) \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "データ $ y $ の条件付き確率が出てきた。\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "p(y \\mid \\mu, \\sigma) &=& \\prod_{i = 1}^{N} \\mathcal{N}(y_i\\mid \\mu, \\sigma^2) \\\\\n",
    " &=&  \\prod_{i = 1}^{N} \\left(\\frac {1} {\\sqrt{2\\pi\\sigma^2}} exp \\left(- \\frac {(y_i- \\mu)^2} {2\\sigma^2} \\right) \\right)\\\\\n",
    " &\\propto&  \\prod_{i = 1}^{N} exp\\left(-\\frac {\\mu^2} {2\\sigma^2} + \\frac {y_{i} \\mu} {\\sigma^2}\\right)\\\\\n",
    " &=& exp\\left(-\\frac {N} {2\\sigma^2} \\mu^2 + \\frac { \\sum_{i = 1}^{N} y_{i}} {\\sigma^2} \\mu \\right)\\\\\n",
    " &\\propto& \\frac {1} {\\sqrt{2\\pi \\frac{\\sigma^2} {N}}} exp \\left(- \\frac { \\left(\\mu - \\frac {1} {N} \\sum_{i = 1}^{N} y_{i} \\right)^2} {2 \\frac{\\sigma^2} {N}} \\right)        \\\\\n",
    " &=&  \\mathcal{N}\\left( \\frac{1} {N} \\sum_{i=1}^{N} y_i, \\frac {\\sigma^2} {N} \\right) \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "最後の二行で、規格化されていない尤度関数からうまいこと正規分布が得られた。\n",
    "\n",
    "これはつまり、ある確率変数 $ x $ の尤度が exp(x の二次式) であるとき、確率変数 $ x $ は正規分布をとる、ということである。対数尤度が$ x $ の二次式なら正規分布！、と憶えるとよさそう。\n",
    "\n",
    "このようにして $ \\mu $ の条件付き確率がわかった。\n",
    "\n",
    "$$ \\mu \\mid \\sigma, y \\sim \\mathcal{N}\\left( \\frac{1} {N} \\sum_{i=1}^{N} y_i, \\frac {\\sigma^2} {N} \\right)$$\n",
    "\n",
    " $ \\mu $ のサンプリングコード、正規分布からのサンプリングなのでとても簡単。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_mu(y, N, s):\n",
    "    mean = np.sum(y) / N\n",
    "    variance = s * s / N\n",
    "    return np.random.normal(mean, np.sqrt(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\sigma $ のサンプリング\n",
    "\n",
    "つづいて $ \\sigma $。 $ \\mu $ と同様に条件付き確率の比例成分として $ y $ の事後分布が得られる。このままでは計算しづらいので、対数尤度 $ lp(\\sigma) $ を解き、$ \\sigma $ がどのような分布に従うか調べる。\n",
    "\n",
    "$$ p(\\sigma \\mid \\mu, y) \\propto p(y \\mid \\mu, \\sigma) $$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "lp(\\sigma) &=& Nlog\\left( \\frac {1} {\\sqrt{2\\pi\\sigma^2}}\\right) -  \\frac {1} {2\\sigma^2} \\sum_{i=1}^N \\left(y_i- \\mu \\right)^2 \\\\\n",
    "&=& -Nlog\\sigma - \\frac {1} {2\\sigma^2} \\sum_{i=1}^N \\left(y_i- \\mu \\right)^2 - \\frac {N} {2} log 2\\pi\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "対数尤度が$ \\sigma $ の二次式ではないので、とりあえず正規分布ではない。ここでガンマ分布([Gamma distribution:wikipedia](https://en.wikipedia.org/wiki/Gamma_distribution))を導入する。\n",
    "\n",
    "ガンマ分布は $ \\alpha$ と $\\beta$ の2パラメタで定義される分布であり、確率変数 $ x $ がガンマ分布に従うとき確率密度は $ p(x\\mid \\alpha, \\beta) \\propto \\beta^\\alpha x^{\\alpha-1}e^{-\\beta x} $ となる。$ x $ の依存項のみを抜き出した対数尤度$ lp(x) $は以下になる。\n",
    "\n",
    "$$ lp(x\\mid \\alpha, \\beta) = (\\alpha - 1)logx - \\beta x $$\n",
    "\n",
    "$ lp(\\sigma) $ と似たような形をしている。ここで、$ \\sigma $ から $\\tau = 1/\\sigma^{2}$ （ $ \\tau $ は精度と呼ばれるもの）への変数変換を行い、\n",
    "\n",
    "$$ lp(\\sigma) = \\frac{N} {2} log\\tau -  \\frac {\\tau} {2} \\sum_{i=1}^N \\left(y_i- \\mu \\right)^2- \\frac {N} {2} log 2\\pi $$\n",
    "\n",
    "より、$ \\tau $ の条件付き確率は以下のガンマ分布に従うことが示せた。\n",
    "\n",
    "$$ \\tau \\mid \\mu, y \\sim \\rm {Gamma} \\left(\\frac{N} {2}+1, \\frac {\\sum_{i=1}^N \\left(y_i- \\mu \\right)^2} {2}  \\right) $$\n",
    "\n",
    "サンプリングコード、$ \\tau $ をサンプリングし、後に $ \\sigma $ に変換する、という実装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_s(y, N, mu):\n",
    "    alpha = N / 2 + 1\n",
    "    residuals = y - mu\n",
    "    beta = np.sum(residuals * residuals) / 2\n",
    "    tau = np.random.gamma(alpha, 1 / beta)\n",
    "    return 1 / np.sqrt(tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプリングの実施\n",
    "\n",
    "条件付き確率が得られたので、Campbellを参考にサンプラを実装する。\n",
    "\n",
    "イテレーションごとに 各 $ y $ の対数尤度（log_likelihoods）を計算している。これはMemorandumを参考にしたもので、WAICというモデルの汎化性能を評価する指標があり、その算出に必要。ここではMemorandumの結果と一致することを確認するために計算している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model1(y, iters, init):\n",
    "    mu = init[\"mu\"]\n",
    "    s = init[\"s\"]\n",
    "    N = len(y)\n",
    "    \n",
    "    trace = np.zeros((iters, 2))\n",
    "    log_likelihoods = np.zeros((iters, N))\n",
    "    \n",
    "    for i in range(iters):\n",
    "        mu = sample_mu(y, N, s)\n",
    "        s = sample_s(y, N, mu)\n",
    "        trace[i, :] = np.array((mu, s))\n",
    "        \n",
    "        norm = sp.stats.norm(mu, s)\n",
    "        log_likelihoods[i, :] = np.array([np.log(norm.pdf(x)) for x in y])\n",
    "    \n",
    "    trace = pd.DataFrame(trace)\n",
    "    trace.columns = ['mu', 's']\n",
    "    \n",
    "    log_likelihoods = pd.DataFrame(log_likelihoods)\n",
    "    \n",
    "    return trace, log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期値。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mu': 9.762700785464943, 's': 71.51893663724195}\n"
     ]
    }
   ],
   "source": [
    "init = {\"mu\": np.random.uniform(-100, 100), \"s\": np.random.uniform(0, 100)}\n",
    "print(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプリング実施。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters = 5000\n",
    "y = np.array([float(x) for x in open(\"points.csv\", \"r\").read().strip().split(\"\\n\")])\n",
    "trace, log_likelihoods = model1(y, iters, init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果\n",
    "\n",
    "サンプリング結果をイテレーションを横軸としてプロットしたものをトレースプロットと呼ぶ。これが一定の範囲内で上下ふらふらしていればひとまずサンプリングは成功と言える。見た感じは良さそう。[^2]\n",
    "\n",
    "[^2]: 見た感じでは許されない状況にある場合は、2つ以上のギブスサンプリングを回して、どれだけ結果が似ているかを評価する。Rhat(potential scale reduction factor)という指標がメジャー。簡単な統計量なのに実装はいくつかあってどれを使えばいいのかよく分からない。どれでもいいのかもしれない。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "traceplot = trace.plot()\n",
    "traceplot.set_xlabel(\"Iteration\")\n",
    "traceplot.set_ylabel(\"Parameter value\")\n",
    "traceplot.get_figure().savefig(\"model1_trace.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメタごとのサンプリング結果のヒストグラム。サンプリング結果の後半だけをtrace_burntに切り出している。ギブスサンプリングを含めたマルコフ連鎖モンテカルロサンプリングでは、サンプリングの序盤は暖機運転扱いで、結果から除外するのが通例であるらしい[^3]。\n",
    "\n",
    "[^3]:他に、サンプリングとサンプリングの間に何回かのイテレーションを挟むことがある。 目的は不明。たくさんイテレーションを回したいが、サンプリング数が多すぎると結果の処理が辛いため？ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_burnt = trace[int(len(trace)/2):]\n",
    "hist_plot = trace_burnt.hist(bins = 30, layout = (1,2))\n",
    "traceplot.get_figure().savefig(\"model1_hist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MemorandumのRコードからWAIC算出関数を移植。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def waic(log_likelihoods):\n",
    "    training_error = -np.log(np.exp(log_likelihoods).mean(axis=0)).mean()\n",
    "    functional_variance_div_N = (np.power(log_likelihoods, 2).mean(axis=0) -\n",
    "                                 np.power(log_likelihoods.mean(axis=0),2)).mean()\n",
    "    return training_error + functional_variance_div_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "統計量等と合わせて表示する。WAICはMemorandumでは1.980なので、よく一致している。成功したらしい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     count      mean       std       min       25%       50%       75%       max\n",
      "mu  2500.0  1.307089  0.173329  0.596588  1.193417  1.307739  1.419458  1.945270\n",
      "s   2500.0  1.728932  0.119035  1.411500  1.644560  1.725093  1.802547  2.212252\n",
      "waic: 1.98051651897\n",
      "np.mean(y): 1.30888736691\n",
      "np.std(y): 1.7214151253\n"
     ]
    }
   ],
   "source": [
    "print(trace_burnt.describe().T)\n",
    "print(\"waic:\", waic(log_likelihoods))\n",
    "print(\"np.mean(y):\", np.mean(y))\n",
    "print(\"np.std(y):\", np.std(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確率モデルの実装(モデル2: 2成分混合モデル)\n",
    "\n",
    "次のモデルに進む。2つの正規分布の混合モデル。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの概要\n",
    "\n",
    "何も考えずにモデルを書くと下になる、と思う。\n",
    "\n",
    "$$y_i \\sim (1-a)  \\mathcal{N}(\\mu_0, \\sigma_0) + a \\mathcal{N}(\\mu_1, \\sigma_1)$$\n",
    "\n",
    "ところがこのモデルはどういじっても条件付き確率が導出できない。どうも正規分布の足し算が難易度高いらしい。無理。\n",
    "\n",
    "困ったのでmixture、gaussian、gibbs等で適当にググると、混合分布モデルの場合はデータ点が属するカテゴリを推定するのがよい、と書いてあるらしい難しい内容の記事・文献がいくつか出てた[^21]。要するにデータ $ y_i $ の属する正規分布カテゴリを表す確率変数 $ z_i $ を導入し、\n",
    "\n",
    "$$ z_i \\sim \\text {Bernoulli} (a) $$\n",
    "\n",
    "$$y_i \\mid z_i \\sim  \\mathcal{N}(\\mu_{z_i}, \\sigma_{z_i}^{2})$$\n",
    "\n",
    "とするといいらしい。Bernoulliは確率aで1、(1-a)で0となる離散確率分布、曲がったコインのトスみたいなもの。これはつまり、今回のテストデータ作成をそのままモデル化したのと同じことである。$ y $ の条件付き確率は、\n",
    "\n",
    "$$p(y_1, \\ldots, y_N \\mid \\mu_1, z_1, \\ldots, z_N) = \\prod_{i = 1}^{N} \\mathcal{N}(y_i\\mid \\mu_{z_i}, \\sigma_{z_i}^{2})$$\n",
    "\n",
    "となり、掛け算なので取り扱いが楽そう。確かに先に進めそうである。 \n",
    "\n",
    "$ \\mu_0 = 0, \\sigma_0 = 1, \\sigma_1 = 1 $ は既知とする。$ \\mu_1 $ と $ a $ の事前分布は以下とする。\n",
    "\n",
    "$$\\mu_1 \\sim \\text{uniform}(-\\infty, \\infty) $$ \n",
    "\n",
    "$$a \\sim \\text{uniform}(0, 1) $$ \n",
    "\n",
    "[^21]: 日本語だと[ベイズ混合モデルにおける近似推論③ ～崩壊型ギブスサンプリング～, 作って遊ぶ機械学習。](http://machine-learning.hatenablog.com/entry/2016/11/03/205317) があった。内容は難しくて3%も理解できない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ \\mu_1 $ のサンプリング\n",
    "\n",
    "$ \\mu_1 $ の条件付き確率。\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "p(\\mu_1 \\mid a, z, y) &\\propto& p(y \\mid \\mu_1, z) \\\\\n",
    "&=& \\prod_{i = 1}^{N} \\mathcal{N}(y_i \\mid \\mu_{z_i}, \\sigma_{z_i}^{2}) \\\\\n",
    "&\\propto& \\prod \\mathcal{N}(y_i\\mid \\mu_{z_i}, \\sigma_{z_i}^{2})\\mid_{z_i=1} \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "考え方はモデル1の $ \\mu $ と同じで、正規分布になる。ただしデータは $ y $ 全体ではなく、 $\\mathcal{N}( \\mu_1, 1) $ の正規分布に属するデータ（$ y_i \\mid_{z_i=1} $）のみを用いる。\n",
    "\n",
    "$$ \\mu_1 \\mid z_1, \\ldots, z_N, y \\sim \\mathcal{N}\\left( \\frac{1} {\\sum^{N}_{i=1} z_i} \\sum y_i\\mid_{z_i=1}, \\frac {1} {\\sum^{N}_{i=1} z_i} \\right)$$\n",
    "\n",
    "サンプリングコード。コード中のzは $ z_i $ の配列だが、要素が0か1なので、 $ y_i\\mid_{z_i=1} $ の総和や総数は以下のように楽ちんに計算できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_mu_1(y, z):\n",
    "    N1 = np.sum(z)\n",
    "    mean = np.sum(y * z) / N1\n",
    "    variance = 1 / N1\n",
    "    return np.random.normal(mean, np.sqrt(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ a $ のサンプリング\n",
    "\n",
    "$a$ の条件付き確率。$ z_i $ は0か1の離散値なので、総和を取ると$ \\mathcal{N} (\\mu_1, 1) $ に属するデータ点の総数になる。\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "p(a \\mid \\mu_1, z, y) &\\propto& \\prod_{i=1}^{N} \\text{Bernoulli}(z_i \\mid a) \\\\\n",
    "&=& (1-a)^{N-\\sum_{i=1}^{N} z_i} a^{\\sum_{i=1}^{N} z_i}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "例によって上の二行目は規格化すると名前のある分布になるのだろうと期待される。ベータ分布 $ \\left( p(x) = \\frac {x^{\\alpha-1}(1-x)^{\\beta-1}} {B(\\alpha, \\beta)}\\right) $ であるようだ。\n",
    "\n",
    "$$\n",
    "p(a\\mid \\mu_1, z, y) = \\text {Beta} \\left( \\sum_{i=1}^{N} z_i + 1, N - \\sum_{i=1}^{N} z_i + 1 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_a(z, a):\n",
    "    alpha = np.sum(z) + 1\n",
    "    beta = len(z) - alpha + 2\n",
    "    return np.random.beta(alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ z_i $のサンプリング\n",
    "\n",
    "$ z_i $ の条件付き確率。$ z_i $ は0 or 1 の離散値なので、尤度さえ分かればサンプリング可能。\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "p(z_i \\mid \\mu_1, a, z_1, \\ldots,z_{i-1}, z_{i+1}, \\ldots, z_N, y) &\\propto& p(y \\mid \\mu_1, z_1, \\ldots, z_N) p(z_i\\mid a) \\\\\n",
    "&\\propto& \\mathcal{N}(y_i \\mid \\mu_{z_i}, \\sigma_{z_i}^{2}) \\times \\text{Bernoulli}(z_i \\mid a) \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "コードでは一つの関数で全 $ z_i $ をサンプリングしている。$ \\mathcal{N}(0, 1) $ は使いまわせるのでグローバルに定義している。\n",
    "zのサンプリング結果を返すのではなく、zの配列を更新しているため、関数名がこれまでと異なる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm0 = sp.stats.norm(0, 1)\n",
    "\n",
    "def sample_and_update_z(y, mu_1, a, z):\n",
    "    norm1 = sp.stats.norm(mu_1, 1)\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        d0 = (1-a) * norm0.pdf(y[i])\n",
    "        d1 = a * norm1.pdf(y[i])\n",
    "        z[i] = 0 if np.random.uniform() * (d0+d1) < d0 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプリングの実施\n",
    "\n",
    "サンプリング実行コードと初期値。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model2(y, iters, init):\n",
    "    mu_1 = init[\"mu_1\"]\n",
    "    a = init[\"a\"]\n",
    "    z = init[\"z\"]\n",
    "    N = len(y)\n",
    "    \n",
    "    trace = np.zeros((iters, 2))\n",
    "    log_likelihoods = np.zeros((iters, N))\n",
    "    \n",
    "    for i in range(iters):\n",
    "        mu_1 = sample_mu_1(y, z)\n",
    "        a = sample_a(z, a)\n",
    "        sample_and_update_z(y, mu_1, a, z)\n",
    "        trace[i, :] = np.array((mu_1, a))\n",
    "        \n",
    "        norm1 = sp.stats.norm(mu_1, 1)\n",
    "        log_likelihoods[i, :] = \\\n",
    "            np.array([np.log((1-a)*norm0.pdf(x) + a*norm1.pdf(x)) for x in y])\n",
    "    \n",
    "    trace = pd.DataFrame(trace)\n",
    "    trace.columns = ['mu_1', 'a']\n",
    "    \n",
    "    log_likelihoods = pd.DataFrame(log_likelihoods)\n",
    "    \n",
    "    return trace, log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mu_1': -99.69811311464245, 'a': 0.7177148886002629, 'z': array([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "       1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
      "       1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "       1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "       0, 1, 0, 1, 0, 0, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "init = {\"mu_1\": np.random.uniform(-100, 100), \"a\": np.random.rand(), \"z\": np.random.randint(0, 2, 100)}\n",
    "print(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプリングを実施。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters = 5000\n",
    "y = np.array([float(x) for x in open(\"points.csv\", \"r\").read().strip().split(\"\\n\")])\n",
    "trace, log_likelihoods = model2(y, iters, init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "traceplot = trace.plot()\n",
    "traceplot.set_xlabel(\"Iteration\")\n",
    "traceplot.set_ylabel(\"Parameter value\")\n",
    "traceplot.get_figure().savefig(\"model2_trace.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_burnt = trace[int(len(trace)/2):]\n",
    "hist_plot = trace_burnt.hist(bins = 30, layout = (1,2))\n",
    "traceplot.get_figure().savefig(\"model2_hist.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "統計量とWAIC。Memorandumでは1.913とのこと。ほぼ同じである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       count      mean       std       min       25%       50%       75%       max\n",
      "mu_1  2500.0  3.083692  0.205765  2.199600  2.940599  3.085343  3.223398  3.768049\n",
      "a     2500.0  0.397030  0.055852  0.214459  0.360779  0.395766  0.433929  0.601889\n",
      "waic: 1.91467884523\n"
     ]
    }
   ],
   "source": [
    "print(trace_burnt.describe().T)\n",
    "print(\"waic:\", waic(log_likelihoods))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rstanでの結果\n",
    "\n",
    "推定結果は良さそうだし、WAICもMemorandumとほぼ同じだ。ついでにMemorandumに書かれたRstanコードを実行し、推定値の統計量を比較する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# results of the mixture normal distribution model with Rstan\n",
      "        mean      se_mean         sd      2.5%       25%       50%       75%     97.5%    n_eff      Rhat\n",
      "mu 3.0925402 0.0012009618 0.19990613 2.7021003 2.9567856 3.0923656 3.2276079 3.4846720 27707.28 0.9999809\n",
      "a  0.3961075 0.0003459609 0.05599797 0.2898431 0.3571346 0.3950372 0.4338865 0.5085656 26199.36 1.0000016\n",
      "WAIC: 1.913930\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "\n",
    "print(check_output([\"Rscript\", \"model2.r\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これもよく一致した。やはりうまくいったようだ。\n",
    "\n",
    "上で使用したRコードはこちら。\n",
    "\n",
    "```R\n",
    "options(width=200)\n",
    "Y <- read.table(\"points.csv\")\n",
    "data   <- list(N=length(Y), Y=Y)\n",
    "\n",
    "model2 <- \"\n",
    "data {\n",
    "  int<lower=1> N;\n",
    "  vector[N] Y;\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real<lower=0, upper=1> a;\n",
    "  real<lower=-50, upper=50> mu;\n",
    "}\n",
    "\n",
    "model {\n",
    "  for(n in 1:N){\n",
    "    target += log_sum_exp(\n",
    "      log(1-a) + normal_lpdf(Y[n] | 0, 1),\n",
    "      log(a) + normal_lpdf(Y[n] | mu, 1)\n",
    "    );\n",
    "  }\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "  vector[N] log_likelihood;\n",
    "  int index;\n",
    "  real y_pred;\n",
    "  for(n in 1:N)\n",
    "    log_likelihood[n] = log_sum_exp(\n",
    "      log(1-a) + normal_lpdf(Y[n] | 0, 1),\n",
    "      log(a) + normal_lpdf(Y[n] | mu, 1)\n",
    "    );\n",
    "  index = bernoulli_rng(a);\n",
    "  y_pred = normal_rng(index == 1 ? mu: 0, 1);\n",
    "}\n",
    "\"\n",
    "\n",
    "sink(file=\"/dev/null\")\n",
    "suppressMessages({\n",
    "  library(rstan)\n",
    "  fit <- stan(model_code=model2, data=data, iter=11000, warmup=1000, seed=123)\n",
    "})\n",
    "sink()\n",
    "cat(\"# results of the mixture normal distribution model with Rstan\\n\")\n",
    "print(summary(fit)$summary[c(\"mu\", \"a\"), ])\n",
    "\n",
    "waic <- function(log_likelihood) {\n",
    "  training_error <- - mean(log(colMeans(exp(log_likelihood))))\n",
    "  functional_variance_div_N <- mean(colMeans(log_likelihood^2) - colMeans(log_likelihood)^2)\n",
    "  waic <- training_error + functional_variance_div_N\n",
    "  return(waic)\n",
    "}\n",
    "\n",
    "cat(sprintf(\"WAIC: %f\", waic(extract(fit)$log_likelihood)))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "おわり。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
